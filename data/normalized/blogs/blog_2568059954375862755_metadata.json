{
    "id": "blog_2568059954375862755",
    "title": "Welcome Gemma - Google‚Äôs new open LLM",
    "authors": [],
    "date": null,
    "doi": null,
    "source": "huggingface.co",
    "tags": [],
    "url": "https://huggingface.co/blog/gemma",
    "raw_text": "Back to Articles\n\nWelcome Gemma - Google‚Äôs new open LLM\n\nPublished\n\t\t\t\tFebruary 21, 2024\n\nUpdate on GitHub\n\nUpvote\n\n25\n\n+19\n\nPhilipp Schmid\n\nphilschmid\n\nFollow\n\nOmar Sanseviero\n\nosanseviero\n\nFollow\n\nPedro Cuenca\n\npcuenq\n\nFollow\n\nAn update to the Gemma models was released two months after this post, see the latest versions\n\nin this collection\n\n.\n\nGemma, a new family of state-of-the-art open LLMs, was released today by Google! It's great to see Google reinforcing its commitment to open-source AI, and we‚Äôre excited to fully support the launch with comprehensive integration in Hugging Face.\n\nGemma comes in two sizes: 7B parameters, for efficient deployment and development on consumer-size GPU and TPU and 2B versions for CPU and on-device applications. Both come in base and instruction-tuned variants.\n\nWe‚Äôve collaborated with Google to ensure the best integration into the Hugging Face ecosystem. You can find the 4 open-access models (2 base models & 2 fine-tuned ones) on the Hub. Among the features and integrations being released, we have:\n\nModels on the Hub\n\n, with their model cards and licenses\n\nü§ó Transformers integration\n\nIntegration with Google Cloud\n\nIntegration with Inference Endpoints\n\nAn example of fine-tuning Gemma on a single GPU with ü§ó¬†TRL\n\nTable of contents\n\nWhat is Gemma?\n\nPrompt format\n\nExploring the Unknowns\n\nDemo\n\nUsing ü§ó¬†Transformers\n\nJAX Weights\n\nIntegration with Google Cloud\n\nIntegration with Inference Endpoints\n\nFine-tuning with ü§ó¬†TRL\n\nAdditional Resources\n\nAcknowledgments\n\nWhat is Gemma?\n\nGemma is a family of 4 new LLM models by Google based on Gemini. It comes in two sizes: 2B and 7B parameters, each with base (pretrained) and instruction-tuned versions. All the variants  can be run on various types of consumer hardware, even without quantization, and have a context length of 8K tokens:\n\ngemma-7b\n\n: Base 7B model.\n\ngemma-7b-it\n\n: Instruction fine-tuned version of the base 7B model.\n\ngemma-2b\n\n: Base 2B model.\n\ngemma-2b-it\n\n: Instruction fine-tuned version of the base 2B model.\n\nA month after the original release, Google released a new version of the instruct models. This version has better coding capabilities, factuality, instruction following and multi-turn quality. The model also is less prone to begin its with \"Sure,\".\n\ngemma-1.1-7b-it\n\ngemma-1.1-2b-it\n\nSo, how good are the Gemma models? Here‚Äôs an overview of the base models and their performance compared to other open models on the\n\nLLM Leaderboard\n\n(higher scores are better):\n\nModel\n\nLicense\n\nCommercial use?\n\nPretraining size [tokens]\n\nLeaderboard  score ‚¨áÔ∏è\n\nLLama 2 70B Chat (reference)\n\nLlama 2 license\n\n‚úÖ\n\n2T\n\n67.87\n\nGemma-7B\n\nGemma license\n\n‚úÖ\n\n6T\n\n63.75\n\nDeciLM-7B\n\nApache 2.0\n\n‚úÖ\n\nunknown\n\n61.55\n\nPHI-2 (2.7B)\n\nMIT\n\n‚úÖ\n\n1.4T\n\n61.33\n\nMistral-7B-v0.1\n\nApache 2.0\n\n‚úÖ\n\nunknown\n\n60.97\n\nLlama 2 7B\n\nLlama 2 license\n\n‚úÖ\n\n2T\n\n54.32\n\nGemma 2B\n\nGemma license\n\n‚úÖ\n\n2T\n\n46.51\n\nGemma 7B is a really strong model, with performance comparable to the best models in the 7B weight, including Mistral 7B. Gemma 2B is an interesting model for its size, but it doesn‚Äôt score as high in the leaderboard as the best capable models with a similar size, such as Phi 2. We are looking forward to receiving feedback from the community about real-world usage!\n\nRecall that the LLM Leaderboard is especially useful for measuring the quality of pretrained models and not so much of the chat ones. We encourage running other benchmarks such as MT Bench, EQ Bench, and the lmsys Arena for the Chat ones!\n\nPrompt format\n\nThe base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning on your own use cases. The Instruct versions have a very simple conversation structure:\n\n<\n\nstart_of_turn\n\n>\n\nuser\nknock knock\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nmodel\nwho is there\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nuser\nLaMDA\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nmodel\nLaMDA who?\n\n<\n\nend_of_turn\n\n>\n\nThis format has to be exactly reproduced for effective use. We‚Äôll later show how easy it is to reproduce the instruct prompt with the chat template available in\n\ntransformers\n\n.\n\nExploring the Unknowns\n\nThe Technical report includes information about the training and evaluation processes of the base models, but there are no extensive details on the dataset‚Äôs composition and preprocessing. We know they were trained with data from various sources, mostly web documents, code, and mathematical texts. The data was filtered to remove CSAM content and PII as well as licensing checks.\n\nSimilarly, for the Gemma instruct models, no details have been shared about the fine-tuning datasets or the hyperparameters associated with SFT and\n\nRLHF\n\n.\n\nDemo\n\nYou can chat with the Gemma Instruct model on Hugging Chat! Check out the link here:\n\nhttps://huggingface.co/chat/models/google/gemma-1.1-7b-it\n\nUsing ü§ó¬†Transformers\n\nWith Transformers\n\nrelease 4.38\n\n, you can use Gemma and leverage all the tools within the Hugging Face ecosystem, such as:\n\ntraining and inference scripts and examples\n\nsafe file format (\n\nsafetensors\n\n)\n\nintegrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\n\nutilities and helpers to run generation with the model\n\nmechanisms to export the models to deploy\n\nIn addition, Gemma models are compatible with\n\ntorch.compile()\n\nwith CUDA graphs, giving them a ~4x speedup at inference time!\n\nTo use Gemma models with transformers, make sure to install a recent version of\n\ntransformers\n\n:\n\npip install --upgrade transformers\n\nThe following snippet shows how to use\n\ngemma-7b-it\n\nwith transformers. It requires about 18 GB of RAM, which includes consumer GPUs such as 3090 or 4090.\n\nfrom\n\ntransformers\n\nimport\n\npipeline\n\nimport\n\ntorch\n\npipe = pipeline(\n\n\"text-generation\"\n\n,\n    model=\n\n\"google/gemma-7b-it\"\n\n,\n    model_kwargs={\n\n\"torch_dtype\"\n\n: torch.bfloat16},\n    device=\n\n\"cuda\"\n\n,\n)\n\nmessages = [\n    {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n:\n\n\"Who are you? Please, answer in pirate-speak.\"\n\n},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=\n\n256\n\n,\n    do_sample=\n\nTrue\n\n,\n    temperature=\n\n0.7\n\n,\n    top_k=\n\n50\n\n,\n    top_p=\n\n0.95\n\n)\nassistant_response = outputs[\n\n0\n\n][\n\n\"generated_text\"\n\n][-\n\n1\n\n][\n\n\"content\"\n\n]\n\nprint\n\n(assistant_response)\n\nAvast me, me hearty. I am a pirate of the high seas, ready to pillage and plunder. Prepare for a tale of adventure and booty!\n\nWe used\n\nbfloat16\n\nbecause that‚Äôs the reference precision and how all evaluations were run. Running in\n\nfloat16\n\nmay be faster on your hardware.\n\nYou can also automatically quantize the model, loading it in 8-bit or even 4-bit mode. 4-bit loading takes about 9 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab. This is how you‚Äôd load the generation pipeline in 4-bit:\n\npipeline =\n\npipeline\n\n(\n\n\"text-generation\"\n\n,\n    model=model,\n    model_kwargs={\n\n\"torch_dtype\"\n\n: torch.\n\nfloat16\n\n,\n\n\"quantization_config\"\n\n: {\n\n\"load_in_4bit\"\n\n:\n\nTrue\n\n}\n    },\n)\n\nFor more details on using the models with transformers, please check\n\nthe model cards\n\n.\n\nJAX Weights\n\nAll the Gemma model variants are available for use with PyTorch, as explained above, or JAX / Flax. To load Flax weights, you need to use the\n\nflax\n\nrevision from the repo, as shown below:\n\nimport\n\njax.numpy\n\nas\n\njnp\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, FlaxGemmaForCausalLM\n\nmodel_id =\n\n\"google/gemma-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side =\n\n\"left\"\n\nmodel, params = FlaxGemmaForCausalLM.from_pretrained(\n        model_id,\n        dtype=jnp.bfloat16,\n        revision=\n\n\"flax\"\n\n,\n        _do_init=\n\nFalse\n\n,\n)\n\ninputs = tokenizer(\n\n\"Valencia and M√°laga are\"\n\n, return_tensors=\n\n\"np\"\n\n, padding=\n\nTrue\n\n)\noutput = model.generate(**inputs, params=params, max_new_tokens=\n\n20\n\n, do_sample=\n\nFalse\n\n)\noutput_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=\n\nTrue\n\n)\n\n['Valencia and M√°laga are two of the most popular tourist destinations in Spain. Both cities boast a rich history, vibrant culture,']\n\nPlease,\n\ncheck out this notebook\n\nfor a comprehensive hands-on walkthrough on how to parallelize JAX inference on Colab TPUs!\n\nIntegration with Google Cloud\n\nYou can deploy and train Gemma on Google Cloud through Vertex AI or Google Kubernetes Engine (GKE), using\n\nText Generation Inference\n\nand Transformers.\n\nTo deploy the Gemma model from Hugging Face, go to the\n\nmodel page\n\nand click on\n\nDeploy -> Google Cloud.\n\nThis will bring you to the Google Cloud Console, where you can 1-click deploy Gemma on Vertex AI or GKE. Text Generation Inference powers Gemma on Google Cloud and is the first integration as part of our\n\npartnership with Google Cloud.\n\nYou can also access Gemma directly through the Vertex AI Model Garden.\n\nTo Tune the Gemma model from Hugging Face, go to the\n\nmodel page\n\nand click on\n\nTrain -> Google Cloud.\n\nThis will bring you to the Google Cloud Console, where you can access notebooks to tune Gemma on Vertex AI or GKE.\n\nThese integrations mark the first offerings we are launching together as a\n\nresult of our collaborative partnership with Google.\n\nStay tuned for more!\n\nIntegration with Inference Endpoints\n\nYou can deploy Gemma on Hugging Face's\n\nInference Endpoints\n\n, which uses Text Generation Inference as the backend.\n\nText Generation Inference\n\nis a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\n\nTo deploy a Gemma model, go to the\n\nmodel page\n\nand click on the\n\nDeploy -> Inference Endpoints\n\nwidget. You can learn more about\n\nDeploying LLMs with Hugging Face Inference Endpoints\n\nin a previous blog post. Inference Endpoints supports\n\nMessages API\n\nthrough Text Generation Inference, which allows you to switch from another closed model to an open one by simply changing the URL.\n\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\n\nclient = OpenAI(\n    base_url=\n\n\"<ENDPOINT_URL>\"\n\n+\n\n\"/v1/\"\n\n,\n\n# replace with your endpoint url\n\napi_key=\n\n\"<HF_API_TOKEN>\"\n\n,\n\n# replace with your token\n\n)\nchat_completion = client.chat.completions.create(\n    model=\n\n\"tgi\"\n\n,\n    messages=[\n        {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n:\n\n\"Why is open-source software important?\"\n\n},\n    ],\n    stream=True,\n    max_tokens=500\n)\n\n# iterate and print stream\n\nfor\n\nmessage\n\nin\n\nchat_completion:\n\nprint\n\n(message.choices[0].delta.content, end=\n\n\"\"\n\n)\n\nFine-tuning with ü§ó¬†TRL\n\nTraining LLMs can be technically and computationally challenging. In this section, we‚Äôll look at the tools available in the Hugging Face ecosystem to efficiently train Gemma on consumer-size GPUs\n\nAn example command to fine-tune Gemma on OpenAssistant‚Äôs\n\nchat dataset\n\ncan be found below. We use 4-bit quantization and\n\nQLoRA\n\nto conserve memory to target all the attention blocks' linear layers.\n\nFirst, install the nightly version of ü§ó TRL and clone the repo to access the\n\ntraining script\n\n:\n\npip install -U transformers trl peft bitsandbytes\ngit clone\n\nhttps\n\n:\n\n//github.com/huggingface/trl\n\ncd trl\n\nThen you can run the script:\n\naccelerate launch --config_file examples/accelerate_configs/multi_gpu.\n\nyaml\n\n--num_processes=\n\n1\n\n\\\n    examples/scripts/sft.\n\npy\n\n\\\n    --model_name google/gemma-7b \\\n    --dataset_name\n\nOpenAssistant\n\n/oasst_top1_2023-\n\n08\n\n-\n\n25\n\n\\\n    --per_device_train_batch_size\n\n2\n\n\\\n    --gradient_accumulation_steps\n\n1\n\n\\\n    --learning_rate\n\n2e-4\n\n\\\n    --save_steps\n\n20_000\n\n\\\n    --use_peft \\\n    --lora_r\n\n16\n\n--lora_alpha\n\n32\n\n\\\n    --lora_target_modules q_proj k_proj v_proj o_proj \\\n    --load_in_4bit \\\n    --output_dir gemma-finetuned-openassistant\n\nThis takes about 9 hours to train on a single A10G, but can be easily parallelized by tweaking\n\n--num_processes\n\nto the number of GPUs you have available.\n\nAdditional Resources\n\nModels on the Hub\n\nOpen LLM\n\nLeaderboard\n\nChat demo on Hugging Chat\n\nOfficial Gemma Blog\n\nGemma Product Page\n\nVertex AI model garden link\n\nGoogle Notebook\n\nAcknowledgments\n\nReleasing such models with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including\n\nCl√©mentine\n\nand\n\nEleuther Evaluation Harness\n\nfor LLM evaluations;\n\nOlivier\n\nand\n\nDavid\n\nfor Text Generation Inference Support;\n\nSimon\n\nfor developing the new access control features on Hugging Face;\n\nArthur\n\n,\n\nYounes\n\n, and\n\nSanchit\n\nfor integrating Gemma into transformers;\n\nMorgan\n\nfor integrating Gemma into optimum-nvidia (coming);\n\nNathan\n\n,\n\nVictor\n\n, and\n\nMishig\n\nfor making Gemma available in Hugging Chat.\n\nAnd Thank you to the Google Team for releasing Gemma and making it available to the open-source AI community!\n\nMore Articles from our Blog\n\nnlp\n\ncommunity\n\nresearch\n\nGoogle releases Gemma 2 2B, ShieldGemma and Gemma Scope\n\nXenova, pcuenq, et. al.\n\n60\n\nJuly 30, 2024\n\nXenova, pcuenq, reach-vb, et. al.\n\nnlp\n\ncommunity\n\nresearch\n\nWelcome Gemma 2 - Google‚Äôs new open LLM\n\n+2\n\nphilschmid, et. al.\n\n132\n\nJune 26, 2024\n\nphilschmid, osanseviero, et. al.\n\nCommunity\n\nEdit\n\nPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or\n\nclicking here\n\n.\n\nTap or paste here to upload images\n\nComment\n\n¬∑\n\nSign up\n\nor\n\nlog in\n\nto comment\n\nUpvote\n\n25\n\n+13",
    "abstract": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
    "first_paragraph": "An update to the Gemma models was released two months after this post, see the latest versions",
    "retrieved_at": "2025-12-08T22:40:45.705278",
    "content_type": "blog",
    "content_length": 13519
}