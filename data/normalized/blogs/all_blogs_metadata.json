[
    {
        "id": "blog_2568059954375862755",
        "title": "Welcome Gemma - Google‚Äôs new open LLM",
        "authors": [],
        "date": null,
        "doi": null,
        "source": "huggingface.co",
        "tags": [],
        "url": "https://huggingface.co/blog/gemma",
        "raw_text": "Back to Articles\n\nWelcome Gemma - Google‚Äôs new open LLM\n\nPublished\n\t\t\t\tFebruary 21, 2024\n\nUpdate on GitHub\n\nUpvote\n\n25\n\n+19\n\nPhilipp Schmid\n\nphilschmid\n\nFollow\n\nOmar Sanseviero\n\nosanseviero\n\nFollow\n\nPedro Cuenca\n\npcuenq\n\nFollow\n\nAn update to the Gemma models was released two months after this post, see the latest versions\n\nin this collection\n\n.\n\nGemma, a new family of state-of-the-art open LLMs, was released today by Google! It's great to see Google reinforcing its commitment to open-source AI, and we‚Äôre excited to fully support the launch with comprehensive integration in Hugging Face.\n\nGemma comes in two sizes: 7B parameters, for efficient deployment and development on consumer-size GPU and TPU and 2B versions for CPU and on-device applications. Both come in base and instruction-tuned variants.\n\nWe‚Äôve collaborated with Google to ensure the best integration into the Hugging Face ecosystem. You can find the 4 open-access models (2 base models & 2 fine-tuned ones) on the Hub. Among the features and integrations being released, we have:\n\nModels on the Hub\n\n, with their model cards and licenses\n\nü§ó Transformers integration\n\nIntegration with Google Cloud\n\nIntegration with Inference Endpoints\n\nAn example of fine-tuning Gemma on a single GPU with ü§ó¬†TRL\n\nTable of contents\n\nWhat is Gemma?\n\nPrompt format\n\nExploring the Unknowns\n\nDemo\n\nUsing ü§ó¬†Transformers\n\nJAX Weights\n\nIntegration with Google Cloud\n\nIntegration with Inference Endpoints\n\nFine-tuning with ü§ó¬†TRL\n\nAdditional Resources\n\nAcknowledgments\n\nWhat is Gemma?\n\nGemma is a family of 4 new LLM models by Google based on Gemini. It comes in two sizes: 2B and 7B parameters, each with base (pretrained) and instruction-tuned versions. All the variants  can be run on various types of consumer hardware, even without quantization, and have a context length of 8K tokens:\n\ngemma-7b\n\n: Base 7B model.\n\ngemma-7b-it\n\n: Instruction fine-tuned version of the base 7B model.\n\ngemma-2b\n\n: Base 2B model.\n\ngemma-2b-it\n\n: Instruction fine-tuned version of the base 2B model.\n\nA month after the original release, Google released a new version of the instruct models. This version has better coding capabilities, factuality, instruction following and multi-turn quality. The model also is less prone to begin its with \"Sure,\".\n\ngemma-1.1-7b-it\n\ngemma-1.1-2b-it\n\nSo, how good are the Gemma models? Here‚Äôs an overview of the base models and their performance compared to other open models on the\n\nLLM Leaderboard\n\n(higher scores are better):\n\nModel\n\nLicense\n\nCommercial use?\n\nPretraining size [tokens]\n\nLeaderboard  score ‚¨áÔ∏è\n\nLLama 2 70B Chat (reference)\n\nLlama 2 license\n\n‚úÖ\n\n2T\n\n67.87\n\nGemma-7B\n\nGemma license\n\n‚úÖ\n\n6T\n\n63.75\n\nDeciLM-7B\n\nApache 2.0\n\n‚úÖ\n\nunknown\n\n61.55\n\nPHI-2 (2.7B)\n\nMIT\n\n‚úÖ\n\n1.4T\n\n61.33\n\nMistral-7B-v0.1\n\nApache 2.0\n\n‚úÖ\n\nunknown\n\n60.97\n\nLlama 2 7B\n\nLlama 2 license\n\n‚úÖ\n\n2T\n\n54.32\n\nGemma 2B\n\nGemma license\n\n‚úÖ\n\n2T\n\n46.51\n\nGemma 7B is a really strong model, with performance comparable to the best models in the 7B weight, including Mistral 7B. Gemma 2B is an interesting model for its size, but it doesn‚Äôt score as high in the leaderboard as the best capable models with a similar size, such as Phi 2. We are looking forward to receiving feedback from the community about real-world usage!\n\nRecall that the LLM Leaderboard is especially useful for measuring the quality of pretrained models and not so much of the chat ones. We encourage running other benchmarks such as MT Bench, EQ Bench, and the lmsys Arena for the Chat ones!\n\nPrompt format\n\nThe base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning on your own use cases. The Instruct versions have a very simple conversation structure:\n\n<\n\nstart_of_turn\n\n>\n\nuser\nknock knock\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nmodel\nwho is there\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nuser\nLaMDA\n\n<\n\nend_of_turn\n\n>\n\n<\n\nstart_of_turn\n\n>\n\nmodel\nLaMDA who?\n\n<\n\nend_of_turn\n\n>\n\nThis format has to be exactly reproduced for effective use. We‚Äôll later show how easy it is to reproduce the instruct prompt with the chat template available in\n\ntransformers\n\n.\n\nExploring the Unknowns\n\nThe Technical report includes information about the training and evaluation processes of the base models, but there are no extensive details on the dataset‚Äôs composition and preprocessing. We know they were trained with data from various sources, mostly web documents, code, and mathematical texts. The data was filtered to remove CSAM content and PII as well as licensing checks.\n\nSimilarly, for the Gemma instruct models, no details have been shared about the fine-tuning datasets or the hyperparameters associated with SFT and\n\nRLHF\n\n.\n\nDemo\n\nYou can chat with the Gemma Instruct model on Hugging Chat! Check out the link here:\n\nhttps://huggingface.co/chat/models/google/gemma-1.1-7b-it\n\nUsing ü§ó¬†Transformers\n\nWith Transformers\n\nrelease 4.38\n\n, you can use Gemma and leverage all the tools within the Hugging Face ecosystem, such as:\n\ntraining and inference scripts and examples\n\nsafe file format (\n\nsafetensors\n\n)\n\nintegrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\n\nutilities and helpers to run generation with the model\n\nmechanisms to export the models to deploy\n\nIn addition, Gemma models are compatible with\n\ntorch.compile()\n\nwith CUDA graphs, giving them a ~4x speedup at inference time!\n\nTo use Gemma models with transformers, make sure to install a recent version of\n\ntransformers\n\n:\n\npip install --upgrade transformers\n\nThe following snippet shows how to use\n\ngemma-7b-it\n\nwith transformers. It requires about 18 GB of RAM, which includes consumer GPUs such as 3090 or 4090.\n\nfrom\n\ntransformers\n\nimport\n\npipeline\n\nimport\n\ntorch\n\npipe = pipeline(\n\n\"text-generation\"\n\n,\n    model=\n\n\"google/gemma-7b-it\"\n\n,\n    model_kwargs={\n\n\"torch_dtype\"\n\n: torch.bfloat16},\n    device=\n\n\"cuda\"\n\n,\n)\n\nmessages = [\n    {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n:\n\n\"Who are you? Please, answer in pirate-speak.\"\n\n},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=\n\n256\n\n,\n    do_sample=\n\nTrue\n\n,\n    temperature=\n\n0.7\n\n,\n    top_k=\n\n50\n\n,\n    top_p=\n\n0.95\n\n)\nassistant_response = outputs[\n\n0\n\n][\n\n\"generated_text\"\n\n][-\n\n1\n\n][\n\n\"content\"\n\n]\n\nprint\n\n(assistant_response)\n\nAvast me, me hearty. I am a pirate of the high seas, ready to pillage and plunder. Prepare for a tale of adventure and booty!\n\nWe used\n\nbfloat16\n\nbecause that‚Äôs the reference precision and how all evaluations were run. Running in\n\nfloat16\n\nmay be faster on your hardware.\n\nYou can also automatically quantize the model, loading it in 8-bit or even 4-bit mode. 4-bit loading takes about 9 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab. This is how you‚Äôd load the generation pipeline in 4-bit:\n\npipeline =\n\npipeline\n\n(\n\n\"text-generation\"\n\n,\n    model=model,\n    model_kwargs={\n\n\"torch_dtype\"\n\n: torch.\n\nfloat16\n\n,\n\n\"quantization_config\"\n\n: {\n\n\"load_in_4bit\"\n\n:\n\nTrue\n\n}\n    },\n)\n\nFor more details on using the models with transformers, please check\n\nthe model cards\n\n.\n\nJAX Weights\n\nAll the Gemma model variants are available for use with PyTorch, as explained above, or JAX / Flax. To load Flax weights, you need to use the\n\nflax\n\nrevision from the repo, as shown below:\n\nimport\n\njax.numpy\n\nas\n\njnp\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, FlaxGemmaForCausalLM\n\nmodel_id =\n\n\"google/gemma-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side =\n\n\"left\"\n\nmodel, params = FlaxGemmaForCausalLM.from_pretrained(\n        model_id,\n        dtype=jnp.bfloat16,\n        revision=\n\n\"flax\"\n\n,\n        _do_init=\n\nFalse\n\n,\n)\n\ninputs = tokenizer(\n\n\"Valencia and M√°laga are\"\n\n, return_tensors=\n\n\"np\"\n\n, padding=\n\nTrue\n\n)\noutput = model.generate(**inputs, params=params, max_new_tokens=\n\n20\n\n, do_sample=\n\nFalse\n\n)\noutput_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=\n\nTrue\n\n)\n\n['Valencia and M√°laga are two of the most popular tourist destinations in Spain. Both cities boast a rich history, vibrant culture,']\n\nPlease,\n\ncheck out this notebook\n\nfor a comprehensive hands-on walkthrough on how to parallelize JAX inference on Colab TPUs!\n\nIntegration with Google Cloud\n\nYou can deploy and train Gemma on Google Cloud through Vertex AI or Google Kubernetes Engine (GKE), using\n\nText Generation Inference\n\nand Transformers.\n\nTo deploy the Gemma model from Hugging Face, go to the\n\nmodel page\n\nand click on\n\nDeploy -> Google Cloud.\n\nThis will bring you to the Google Cloud Console, where you can 1-click deploy Gemma on Vertex AI or GKE. Text Generation Inference powers Gemma on Google Cloud and is the first integration as part of our\n\npartnership with Google Cloud.\n\nYou can also access Gemma directly through the Vertex AI Model Garden.\n\nTo Tune the Gemma model from Hugging Face, go to the\n\nmodel page\n\nand click on\n\nTrain -> Google Cloud.\n\nThis will bring you to the Google Cloud Console, where you can access notebooks to tune Gemma on Vertex AI or GKE.\n\nThese integrations mark the first offerings we are launching together as a\n\nresult of our collaborative partnership with Google.\n\nStay tuned for more!\n\nIntegration with Inference Endpoints\n\nYou can deploy Gemma on Hugging Face's\n\nInference Endpoints\n\n, which uses Text Generation Inference as the backend.\n\nText Generation Inference\n\nis a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\n\nTo deploy a Gemma model, go to the\n\nmodel page\n\nand click on the\n\nDeploy -> Inference Endpoints\n\nwidget. You can learn more about\n\nDeploying LLMs with Hugging Face Inference Endpoints\n\nin a previous blog post. Inference Endpoints supports\n\nMessages API\n\nthrough Text Generation Inference, which allows you to switch from another closed model to an open one by simply changing the URL.\n\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\n\nclient = OpenAI(\n    base_url=\n\n\"<ENDPOINT_URL>\"\n\n+\n\n\"/v1/\"\n\n,\n\n# replace with your endpoint url\n\napi_key=\n\n\"<HF_API_TOKEN>\"\n\n,\n\n# replace with your token\n\n)\nchat_completion = client.chat.completions.create(\n    model=\n\n\"tgi\"\n\n,\n    messages=[\n        {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n:\n\n\"Why is open-source software important?\"\n\n},\n    ],\n    stream=True,\n    max_tokens=500\n)\n\n# iterate and print stream\n\nfor\n\nmessage\n\nin\n\nchat_completion:\n\nprint\n\n(message.choices[0].delta.content, end=\n\n\"\"\n\n)\n\nFine-tuning with ü§ó¬†TRL\n\nTraining LLMs can be technically and computationally challenging. In this section, we‚Äôll look at the tools available in the Hugging Face ecosystem to efficiently train Gemma on consumer-size GPUs\n\nAn example command to fine-tune Gemma on OpenAssistant‚Äôs\n\nchat dataset\n\ncan be found below. We use 4-bit quantization and\n\nQLoRA\n\nto conserve memory to target all the attention blocks' linear layers.\n\nFirst, install the nightly version of ü§ó TRL and clone the repo to access the\n\ntraining script\n\n:\n\npip install -U transformers trl peft bitsandbytes\ngit clone\n\nhttps\n\n:\n\n//github.com/huggingface/trl\n\ncd trl\n\nThen you can run the script:\n\naccelerate launch --config_file examples/accelerate_configs/multi_gpu.\n\nyaml\n\n--num_processes=\n\n1\n\n\\\n    examples/scripts/sft.\n\npy\n\n\\\n    --model_name google/gemma-7b \\\n    --dataset_name\n\nOpenAssistant\n\n/oasst_top1_2023-\n\n08\n\n-\n\n25\n\n\\\n    --per_device_train_batch_size\n\n2\n\n\\\n    --gradient_accumulation_steps\n\n1\n\n\\\n    --learning_rate\n\n2e-4\n\n\\\n    --save_steps\n\n20_000\n\n\\\n    --use_peft \\\n    --lora_r\n\n16\n\n--lora_alpha\n\n32\n\n\\\n    --lora_target_modules q_proj k_proj v_proj o_proj \\\n    --load_in_4bit \\\n    --output_dir gemma-finetuned-openassistant\n\nThis takes about 9 hours to train on a single A10G, but can be easily parallelized by tweaking\n\n--num_processes\n\nto the number of GPUs you have available.\n\nAdditional Resources\n\nModels on the Hub\n\nOpen LLM\n\nLeaderboard\n\nChat demo on Hugging Chat\n\nOfficial Gemma Blog\n\nGemma Product Page\n\nVertex AI model garden link\n\nGoogle Notebook\n\nAcknowledgments\n\nReleasing such models with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including\n\nCl√©mentine\n\nand\n\nEleuther Evaluation Harness\n\nfor LLM evaluations;\n\nOlivier\n\nand\n\nDavid\n\nfor Text Generation Inference Support;\n\nSimon\n\nfor developing the new access control features on Hugging Face;\n\nArthur\n\n,\n\nYounes\n\n, and\n\nSanchit\n\nfor integrating Gemma into transformers;\n\nMorgan\n\nfor integrating Gemma into optimum-nvidia (coming);\n\nNathan\n\n,\n\nVictor\n\n, and\n\nMishig\n\nfor making Gemma available in Hugging Chat.\n\nAnd Thank you to the Google Team for releasing Gemma and making it available to the open-source AI community!\n\nMore Articles from our Blog\n\nnlp\n\ncommunity\n\nresearch\n\nGoogle releases Gemma 2 2B, ShieldGemma and Gemma Scope\n\nXenova, pcuenq, et. al.\n\n60\n\nJuly 30, 2024\n\nXenova, pcuenq, reach-vb, et. al.\n\nnlp\n\ncommunity\n\nresearch\n\nWelcome Gemma 2 - Google‚Äôs new open LLM\n\n+2\n\nphilschmid, et. al.\n\n132\n\nJune 26, 2024\n\nphilschmid, osanseviero, et. al.\n\nCommunity\n\nEdit\n\nPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or\n\nclicking here\n\n.\n\nTap or paste here to upload images\n\nComment\n\n¬∑\n\nSign up\n\nor\n\nlog in\n\nto comment\n\nUpvote\n\n25\n\n+13",
        "abstract": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
        "first_paragraph": "An update to the Gemma models was released two months after this post, see the latest versions",
        "retrieved_at": "2025-12-08T22:40:45.705278",
        "content_type": "blog",
        "content_length": 13519
    },
    {
        "id": "blog_6179328093277316072",
        "title": "Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart",
        "authors": [],
        "date": "2023-05-02T07:44:03-08:00",
        "doi": null,
        "source": "aws.amazon.com",
        "tags": [
            "Amazon SageMaker"
        ],
        "url": "https://aws.amazon.com/fr/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/",
        "raw_text": "Artificial Intelligence\n\nQuestion answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart\n\nToday, we announce the availability of sample notebooks that demonstrate question answering tasks using a Retrieval Augmented Generation (RAG)-based approach with large language models (LLMs) in\n\nAmazon SageMaker JumpStart\n\n. Text generation using RAG with LLMs enables you to generate domain-specific text outputs by supplying specific external data as part of the context fed to LLMs.\n\nJumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. JumpStart provides many pre-trained language models called\n\nfoundation models\n\nthat can help you perform tasks such as article summarization, question answering, and conversation generation and image generation.\n\nIn this post, we describe RAG and its advantages, and demonstrate how to quickly get started by using a sample notebook to solve a question answering task using RAG implementation with LLMs in Jumpstart. We demonstrate two approaches:\n\nHow to solve the problem with the open-sourced\n\nLangChain\n\nlibrary and\n\nAmazon SageMaker\n\nendpoints in a few lines of code\n\nHow to use the SageMaker KNN algorithm to perform semantic searching for large-scale data using SageMaker endpoints\n\nLLMS and constraints\n\nLLMs are trained on large amounts of unstructured data and are great at general text generation. LLMs can store factual knowledge by training their parameters on a large corpus of natural language data.\n\nThere are a few limitations of using off-the-shelf pre-trained LLMs:\n\nThey‚Äôre usually trained offline, making the model agnostic to the latest information (for example, a chatbot trained from 2011‚Äì2018 has no information about COVID-19).\n\nThey make predictions by only looking up information stored in its parameters, leading to inferior interpretability.\n\nThey‚Äôre mostly trained on general domain corpora, making them less effective on domain-specific tasks. There are scenarios when you want models to generate text based on specific data rather than generic data. For example, a health insurance company may want their question answering bot to answer questions using the latest information stored in their enterprise document repository or database, so the answers are accurate and reflect their unique business rules.\n\nCurrently, there are two popular ways to reference specific data in LLMs:\n\nInsert data as context in the model prompt as a way to provide the information that the model can use while creating the result\n\nFine-tune the model by providing a file with prompt and completion pairs\n\nThe challenge of the context-based approach is that models come with limited context size, and including all the documents as context may not fit into the allowed context size of the model. Depending on the model used, there may also be additional cost for larger context.\n\nFor the approach of fine-tuning, generating the right formatted information is time consuming and involves cost. In addition, if external data used for fine-tuning changes frequently, it would imply frequent fine-tunings and retraining are needed to create accurate results. Frequent training impacts speed to market and adds to the overall solution cost.\n\nTo demonstrate these constraints, we used an LLM Flan T5 XXL model and asked the following question:\n\nquestion = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n\nCode\n\nWe get the following response:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \nthe Managed Spot Training is a subscriptions product available for the following instances: Data Science Virtual Machine (DSVM), DSVM High, and DSVM Low.\n\"\"\"\n\nCode\n\nAs you can see, the response is not accurate. The correct answer should be all SageMaker instances support Managed Spot Training.\n\nWe tried the same question but with additional context passed along with the question:\n\nquestion + context + prompt = \"\"\"\nAnswer based on context:\n\nManaged Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n\nWhich instances can I use with Managed Spot Training in SageMaker?\n\"\"\"\n\nCode\n\nWe got the following response this time:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \ninstances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nThe response is better but still not accurate. However, in real production use cases, users may send various queries, and to provide accurate responses, you may want to include all or most of the available information as part of the static context to create accurate responses. Therefore, with this approach, we may hit the context size limitation constraint because even non-relevant information for the question asked is sent as part of the context. This is where you can use the RAG-based approach to create scalable and accurate responses for a user‚Äôs queries.\n\nRetrieval Augmented Generation\n\nTo solve the constraints we discussed, we can use Retrieval Augmented Generation (RAG) with LLMs. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. RAG models were introduced by\n\nLewis et al. in 2020\n\nas a model where parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nIn RAG, the external data can come from multiple data sources, such as a document repository, databases, or APIs. The first step is to convert the documents and the user query in the format so they can be compared and relevancy search can be performed. To make the formats comparable for doing relevancy search, a document collection (knowledge library) and the user-submitted query are converted to numerical representation using embedding language models. The embeddings are essentially numerical representations of concept in text. Next, based on the embedding of user query, its relevant text is identified in the document collection by a similarity search in the embedding space. Then the prompt provided by the user is appended with relevant text that was searched and it‚Äôs added to the context. The prompt is now sent to the LLM and because the context has relevant external data along with the original prompt, the model output is relevant and accurate.\n\nTo maintain up-to-date information for the reference documents, you can asynchronously update the documents and update embedding representation of the documents. This way, the updated documents will be used to generate answers for future questions to provide accurate responses.\n\nThe following diagram shows the conceptual flow of using RAG with LLMs.\n\nIn this post, we demonstrate how to implement a question answering application with the following steps:\n\nGenerate embedding for each of document in the knowledge library with a SageMaker GPT-J-6B embedding model.\n\nIdentify the top K most relevant documents based on the user query.\n\nFor your query, generate the embedding of the query using the same embedding model.\n\nSearch the indexes of the top K most relevant documents in the embedding space using an in-memory FAISS search.\n\nUse the indexes to retrieve the corresponding documents.\n\nUse the retrieved relevant documents as context with the prompt and question, and send them to the SageMaker LLM to generate the response.\n\nWe demonstrate the following approaches:\n\nHow to solve a question answering task with SageMaker LLMs and embedding endpoints and the open-sourced library LangChain in a few lines of code. In particular, we use two SageMaker endpoints for the LLM (Flan T5 XXL) and embedding model (GPT-J 6B), and the vector database used is in-memory\n\nFAISS\n\n. For more details, see the\n\nGitHub repo\n\n.\n\nIf the in-memory FAISS doesn‚Äôt fit into your large dataset, we provide you with a\n\nSageMaker KNN algorithm\n\nto perform the semantic search, which also uses FAISS as the underlying searching algorithm. For details, see the\n\nGitHub repo\n\n.\n\nThe following diagram depicts the solution architecture.\n\nJumpStart RAG-based implementation notebook with LangChain\n\nLangChain\n\nis an open-source framework for developing applications powered by language models. LangChain provides a generic interface for many different LLMs. It also makes it easier for developers to chain various LLMs together and build powerful applications. LangChain provides a standard interface for memory and a collection of memory implementations to persist the state between calls of agents or chains.\n\nLangChain has many other utility features that can add to developer productivity. These features include a prompt template that helps customize prompts using variables in the prompt template, agents to build end-to-end applications, indexes for search and retrieval steps of the chain, and much more. To further explore LangChain capabilities, refer to the\n\nLangChain documentation\n\n.\n\nCreate LLM Model\n\nAs a first step, deploy the JumpStart LLM model of your choice. In this demo, we use a Jumpstart Flan T5 XXL model endpoint. For deployment instructions, refer to\n\nZero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart\n\n. Based on your use case, you can also deploy other instruction-tuned models like\n\nFlan T5 UL2\n\nor\n\nBloomZ 7B1\n\n. For details, see the\n\nexample notebook\n\n.\n\nTo use the SageMaker LLM endpoint with LangChain, we use\n\nlangchain.llms.sagemaker_endpoint.SagemakerEndpoint\n\n, which abstracts the SageMaker LLM endpoint. We need to perform a transformation for the request and response payload as shown in the following code for the LangChain SageMaker integration. Note that you may need to adjust the code in\n\nContentHandler\n\nbased on the\n\ncontent_type\n\nand accepts format of the LLM model that you choose to use.\n\nfrom\n\nlangchain\n\n.\n\nllms\n\n.\n\nsagemaker_endpoint\n\nimport\n\nSagemakerEndpoint\n\nclass\n\nContentHandler\n\n(\n\nContentHandlerBase\n\n)\n\n:\n\ncontent_type\n\n=\n\n\"application/json\"\n\naccepts\n\n=\n\n\"application/json\"\n\ndef\n\ntransform_input\n\n(\n\nself\n\n,\n\nprompt\n\n:\n\nstr\n\n,\n\nmodel_kwargs\n\n=\n\n{\n\n}\n\n)\n\n-\n\n>\n\nbytes\n\n:\n\ninput_str\n\n=\n\njson\n\n.\n\ndumps\n\n(\n\n{\n\n\"text_inputs\"\n\n:\n\nprompt\n\n,\n\n**\n\nmodel_kwargs\n\n}\n\n)\n\nreturn\n\ninput_str\n\n.\n\nencode\n\n(\n\n\"utf-8\"\n\n)\n\ndef\n\ntransform_output\n\n(\n\nself\n\n,\n\noutput\n\n:\n\nbytes\n\n)\n\n-\n\n>\n\nstr\n\n:\n\nresponse_json\n\n=\n\njson\n\n.\n\nloads\n\n(\n\noutput\n\n.\n\nread\n\n(\n\n)\n\n.\n\ndecode\n\n(\n\n\"utf-8\"\n\n)\n\n)\n\nreturn\n\nresponse_json\n\n[\n\n\"generated_texts\"\n\n]\n\n[\n\n0\n\n]\n\ncontent_handler\n\n=\n\nContentHandler\n\n(\n\n)\n\nsm_llm\n\n=\n\nSagemakerEndpoint\n\n(\n\nendpoint_name\n\n=\n\n_MODEL_CONFIG_\n\n[\n\n\"huggingface-text2text-flan-t5-xxl\"\n\n]\n\n[\n\n\"endpoint_name\"\n\n]\n\n,\n\nregion_name\n\n=\n\naws_region\n\n,\n\nmodel_kwargs\n\n=\n\nparameters\n\n,\n\ncontent_handler\n\n=\n\ncontent_handler\n\n,\n\n)\n\nPython\n\nCreate the embedding model\n\nNext, we need to get our embedded model ready. We deploy the\n\nGPT-J 6B\n\nmodel as the embedding model. If you‚Äôre using a JumpStart embedding model, you need to customize the LangChain SageMaker endpoint embedding class and transform the model request and response to integrate with LangChain. For a detailed implementation, refer to the\n\nGitHub repo\n\n.\n\nembeddings\n\n=\n\nSagemakerEndpointEmbeddingsJumpStart\n\n(\n\nendpoint_name\n\n=\n\n_MODEL_CONFIG_\n\n[\n\n\"huggingface-textembedding-gpt-j-6b\"\n\n]\n\n[\n\n\"endpoint_name\"\n\n]\n\n,\n\nregion_name\n\n=\n\naws_region\n\n,\n\ncontent_handler\n\n=\n\ncontent_handler\n\n,\n\n)\n\nPython\n\nLoad domain-specific documents using the LangChain document loader and create an index\n\nWe use the\n\nCSVLoader\n\npackage in LangChain to load CSV-formatted documents into the document loader:\n\nloader\n\n=\n\nCSVLoader\n\n(\n\nfile_path\n\n=\n\n\"rag_data/processed_data.csv\"\n\n)\n\ndocuments\n\n=\n\nloader\n\n.\n\nload\n\n(\n\n)\n\nPython\n\nNext, we use TextSplitter to preprocess data for embedding purposes and use the SageMaker embedding model\n\nGPT-J -6B\n\nto create the embedding. We store embedding in a FAISS vector store to create an index. We use this index to find relevant documents that are semantically similar to the user‚Äôs query.\n\nThe following code shows how all these steps are done by the\n\nVectorstoreIndexCreator\n\nclass in just few lines of code in LangChain to create a concise implementation of question answering with RAG:\n\nindex_creator\n\n=\n\nVectorstoreIndexCreator\n\n(\n\nvectorstore_cls\n\n=\n\nFAISS\n\n,\n\nembedding\n\n=\n\nembeddings\n\n,\n\ntext_splitter\n\n=\n\nCharacterTextSplitter\n\n(\n\nchunk_size\n\n=\n\n300\n\n,\n\nchunk_overlap\n\n=\n\n0\n\n)\n\n,\n\n)\n\nindex\n\n=\n\nindex_creator\n\n.\n\nfrom_loaders\n\n(\n\n[\n\nloader\n\n]\n\n)\n\nPython\n\nUse the index to search for relevant context and pass it to the LLM model\n\nNext, use the query method on the created index and pass the user‚Äôs question and SageMaker endpoint LLM. LangChain selects the top four closest documents (K=4) and passes the relevant context extracted from the documents to generate an accurate response. See the following code:\n\nindex.query(question=question, llm=sm_llm)\n\nCode\n\nWe get the following response for the query using the RAG-based approach with Flan T5 XXL:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \nManaged Spot Training can be used with all instances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nThe response looks more accurate compared to the response we got with other approaches that we demonstrated earlier that have no context or static context that may not be always relevant.\n\nAlternate approach to implement RAG with more customization using SageMaker and LangChain\n\nIn this section, we show you another approach to implement RAG using SageMaker and LangChain. This approach offers the flexibility to configure top K parameters for a relevancy search in the documents. It also allows you to use the LangChain feature of\n\nprompt templates\n\n, which allow you to easily parameterize the prompt creation instead of hard coding the prompts.\n\nIn the following code, we explicitly use FAISS to generate embedding for each of the document in the knowledge library with the SageMaker GPT-J-6B embedding model. Then we identify the top K (K=3) most relevant documents based on the user query.\n\ndocsearch = FAISS.from_documents(documents, embeddings)\ndocs = docsearch.similarity_search(question, k=3)\n\nCode\n\nNext, we use a prompt template and chain it with the SageMaker LLM:\n\nprompt_template\n\n=\n\n\"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n\nPROMPT\n\n=\n\nPromptTemplate\n\n(\n\ntemplate\n\n=\n\nprompt_template\n\n,\n\ninput_variables\n\n=\n\n[\n\n\"context\"\n\n,\n\n\"question\"\n\n]\n\n)\n\nchain\n\n=\n\nload_qa_chain\n\n(\n\nllm\n\n=\n\nsm_llm\n\n,\n\nprompt\n\n=\n\nPROMPT\n\n)\n\nPython\n\nWe send the top three (K=3) relevant documents we found as context to the prompt by using a LangChain chain:\n\nresult\n\n=\n\nchain\n\n(\n\n{\n\n\"input_documents\"\n\n:\n\ndocs\n\n,\n\n\"question\"\n\n:\n\nquestion\n\n}\n\n,\n\nreturn_only_outputs\n\n=\n\nTrue\n\n)\n\n[\n\n\"output_text\"\n\n]\n\nPython\n\nWith this approach of RAG implementation, we were able to take advantage of the additional flexibility of LangChain prompt templates and customize the number of documents searched for a relevancy match using the top K hyperparameter.\n\nJumpStart RAG-based implementation notebook with SageMaker KNN\n\nIn this section, we implement the RAG-based approach using the KNN algorithm for finding relevant documents to create enhanced context. In this approach, we‚Äôre not using LangChain, but we use same dataset\n\nAmazon SageMaker FAQs\n\nas knowledge documents, embedding the models GPT-J-6B and LLM Flan T5 XXL just as we did in the previous LangChain approach.\n\nIf you have a large dataset, the\n\nSageMaker KNN algorithm\n\nmay provide you with an effective semantic search. The SageMaker KNN algorithm also uses FAISS as the underlying search algorithm. The notebook for this solution can be found on\n\nGitHub\n\n.\n\nFirst, we deploy the LLM Flan T5 XXL and GPT-J 6B embedding models in the same way as in the previous section. For each record in the knowledge database, we generate an embedding vector using the GPT-J embedding model.\n\nNext, we use a\n\nSageMaker KNN\n\ntraining job to index the embedding of the knowledge data. The underlying algorithm used to index the data is\n\nFAISS\n\n. We want to find the top five most relevant documents, so we set the\n\nTOP_K\n\nvariable to 5. We create the estimator for the KNN algorithm, run the training job, and deploy the KNN model to find indexes of the top five documents matching the query. See the following code:\n\nfrom\n\nsagemaker\n\n.\n\namazon\n\n.\n\namazon_estimator\n\nimport\n\nget_image_uri\n\ndef\n\ntrained_estimator_from_hyperparams\n\n(\n\ns3_train_data\n\n,\n\nhyperparams\n\n,\n\noutput_path\n\n)\n\n:\n\n\"\"\"\n    Create an Estimator from the given hyperparams, fit to training data,\n    and return a deployed predictor\n\n    \"\"\"\n\n# set up the estimator\n\nknn\n\n=\n\nsagemaker\n\n.\n\nestimator\n\n.\n\nEstimator\n\n(\n\nget_image_uri\n\n(\n\nboto3\n\n.\n\nSession\n\n(\n\n)\n\n.\n\nregion_name\n\n,\n\n\"knn\"\n\n)\n\n,\n\naws_role\n\n,\n\ninstance_count\n\n=\n\n1\n\n,\n\ninstance_type\n\n=\n\n\"ml.m5.2xlarge\"\n\n,\n\noutput_path\n\n=\n\noutput_path\n\n,\n\nsagemaker_session\n\n=\n\nsess\n\n,\n\n)\n\nknn\n\n.\n\nset_hyperparameters\n\n(\n\n**\n\nhyperparams\n\n)\n\n# train a model. fit_input contains the locations of the train data\n\nfit_input\n\n=\n\n{\n\n\"train\"\n\n:\n\ns3_train_data\n\n}\n\nknn\n\n.\n\nfit\n\n(\n\nfit_input\n\n)\n\nreturn\n\nknn\n\nhyperparams\n\n=\n\n{\n\n\"feature_dim\"\n\n:\n\ntrain_features\n\n.\n\nshape\n\n[\n\n1\n\n]\n\n,\n\n\"k\"\n\n:\n\nTOP_K\n\n,\n\n\"sample_size\"\n\n:\n\ntrain_features\n\n.\n\nshape\n\n[\n\n0\n\n]\n\n,\n\n\"predictor_type\"\n\n:\n\n\"classifier\"\n\n}\n\noutput_path\n\n=\n\nf\"s3://\n\n{\n\nbucket\n\n}\n\n/\n\n{\n\nprefix\n\n}\n\n/default_example/output\"\n\nknn_estimator\n\n=\n\ntrained_estimator_from_hyperparams\n\n(\n\ns3_train_data\n\n,\n\nhyperparams\n\n,\n\noutput_path\n\n)\n\nPython\n\nNext, we create an embedding representation of the query using the GPT-J-6B embedding model that we used for creating an embedding of the knowledge library documents:\n\nquery_response\n\n=\n\nquery_endpoint_with_json_payload\n\n(\n\nquestion\n\n,\n\nendpoint_name_embed\n\n,\n\ncontent_type\n\n=\n\n\"application/x-text\"\n\n)\n\nquestion_embedding\n\n=\n\nparse_response_text_embed\n\n(\n\nquery_response\n\n)\n\nPython\n\nThen we use the KNN endpoint and pass the embedding of the query to the KNN endpoint to get the indexes of the top K most relevant documents. We use the indexes to retrieve the corresponded textual documents. Next, we concatenate the documents, ensuring the maximum allowed length of context is not exceeded. See the following code:\n\n\"\"\"With maximum sequence length 500, selected top 4 document sections: \n  Managed Spot Training can be used with all instances supported in Amazon SageMaker.\n  Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n  The difference between Savings Plans for Amazon SageMaker and Savings Plans for EC2 is in the services they \n  include. \n  SageMaker Savings Plans apply only to SageMaker ML Instance usage.\n  There are no fixed limits to the size of the dataset you can use for training models with Amazon SageMaker.\n\"\"\"\n\nCode\n\nNow we come to our final step in which we combine the query, prompt, and the context containing text from relevant documents and pass it to the text generation LLM Flan T5 XXL model to generate the answer.\n\nWe get the following response for the query using a RAG-based approach with Flan T5 XXL:\n\n\"\"\"\nFor model: huggingface-text2text-flan-t5-xxl, the generated output is: \n\nManaged Spot Training can be used with all instances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nClean up\n\nMake sure to delete the endpoints that we created in this notebook when not using them to avoid reoccurring cost.\n\nConclusion\n\nIn this post, we demonstrated the implementation of a RAG-based approach with LLMs for question answering tasks using two approaches: LangChain and the built-in KNN algorithm. The RAG-based approach optimizes the accuracy of the text generation using Flan T5 XXL by dynamically providing relevant context that was created by searching a list of documents.\n\nYou can use this these notebooks in SageMaker as is or you may customize them to your needs. To customize, you can use your own set of documents in the knowledge library, use other relevancy search implementations like OpenSearch, and use other embedding models and text generation LLMs available on JumpStart.\n\nWe look forward to seeing what you build on JumpStart using a RAG-based approach!\n\nAbout the authors\n\nDr. Xin Huang\n\nis a Senior Applied Scientist for Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He focuses on developing scalable machine learning algorithms. His research interests are in the area of natural language processing, explainable deep learning on tabular data, and robust analysis of non-parametric space-time clustering. He has published many papers in ACL, ICDM, KDD conferences, and Royal Statistical Society: Series A.\n\nRachna Chadha\n\nis a Principal Solution Architect AI/ML in Strategic Accounts at AWS. Rachna is an optimist who believes that ethical and responsible use of AI can improve society in future and bring economical and social prosperity. In her spare time, Rachna likes spending time with her family, hiking and listening to music.\n\nDr. Kyle Ulrich\n\nis an Applied Scientist with the Amazon SageMaker built-in algorithms team. His research interests include scalable machine learning algorithms, computer vision, time series, Bayesian non-parametrics, and Gaussian processes. His PhD is from Duke University and he has published papers in NeurIPS, Cell, and Neuron.\n\nHemant Singh\n\nis a Machine Learning Engineer with experience in Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He got his masters from Courant Institute of Mathematical Sciences and B.Tech from IIT Delhi. He had experience in working on a diverse range of Machine Learning problems within the domain of natural language processing, computer vision, and time-series analysis.\n\nManas Dadarkar\n\nis a Software Development Manager owning the engineering of the Amazon Forecast service. He is passionate about the applications of machine learning and making ML technologies easily available for everyone to adopt and deploy to production. Outside of work, he has multiple interests including travelling, reading and spending time with friends and family.\n\nDr. Ashish Khetan\n\nis a Senior Applied Scientist with Amazon SageMaker built-in algorithms and helps develop machine learning algorithms. He got his PhD from University of Illinois Urbana-Champaign. He is an active researcher in machine learning and statistical inference, and has published many papers in NeurIPS, ICML, ICLR, JMLR, ACL, and EMNLP conferences.\n\nLike\n\n(0)\n\nShare\n\nComments\n\nLog in to comment\n\nLog in\n\nAWS Podcast\n\nSubscribe for weekly AWS news and interviews\n\nLearn more\n\nAWS Partner Network\n\nFind an APN member to support your cloud business needs\n\nLearn more\n\nAWS Training & Certifications\n\nFree digital courses to help you develop your skills\n\nLearn more",
        "abstract": "Today, we announce the availability of sample notebooks that demonstrate question answering tasks using a Retrieval Augmented Generation (RAG)-based approach with large language models (LLMs) in Amazon SageMaker JumpStart. Text generation using RAG with LLMs enables you to generate domain-specific text outputs by supplying specific external data as part of the context fed to LLMs. [‚Ä¶]",
        "first_paragraph": "Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart",
        "retrieved_at": "2025-12-08T22:41:35.291073",
        "content_type": "blog",
        "content_length": 22793
    }
]