{
    "id": "blog_4204977657293528576",
    "title": "Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart",
    "authors": [],
    "date": "2023-05-02T07:44:03-08:00",
    "doi": null,
    "source": "aws.amazon.com",
    "tags": [
        "Amazon SageMaker"
    ],
    "url": "https://aws.amazon.com/fr/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/",
    "raw_text": "Artificial Intelligence\n\nQuestion answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart\n\nToday, we announce the availability of sample notebooks that demonstrate question answering tasks using a Retrieval Augmented Generation (RAG)-based approach with large language models (LLMs) in\n\nAmazon SageMaker JumpStart\n\n. Text generation using RAG with LLMs enables you to generate domain-specific text outputs by supplying specific external data as part of the context fed to LLMs.\n\nJumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. JumpStart provides many pre-trained language models called\n\nfoundation models\n\nthat can help you perform tasks such as article summarization, question answering, and conversation generation and image generation.\n\nIn this post, we describe RAG and its advantages, and demonstrate how to quickly get started by using a sample notebook to solve a question answering task using RAG implementation with LLMs in Jumpstart. We demonstrate two approaches:\n\nHow to solve the problem with the open-sourced\n\nLangChain\n\nlibrary and\n\nAmazon SageMaker\n\nendpoints in a few lines of code\n\nHow to use the SageMaker KNN algorithm to perform semantic searching for large-scale data using SageMaker endpoints\n\nLLMS and constraints\n\nLLMs are trained on large amounts of unstructured data and are great at general text generation. LLMs can store factual knowledge by training their parameters on a large corpus of natural language data.\n\nThere are a few limitations of using off-the-shelf pre-trained LLMs:\n\nThey’re usually trained offline, making the model agnostic to the latest information (for example, a chatbot trained from 2011–2018 has no information about COVID-19).\n\nThey make predictions by only looking up information stored in its parameters, leading to inferior interpretability.\n\nThey’re mostly trained on general domain corpora, making them less effective on domain-specific tasks. There are scenarios when you want models to generate text based on specific data rather than generic data. For example, a health insurance company may want their question answering bot to answer questions using the latest information stored in their enterprise document repository or database, so the answers are accurate and reflect their unique business rules.\n\nCurrently, there are two popular ways to reference specific data in LLMs:\n\nInsert data as context in the model prompt as a way to provide the information that the model can use while creating the result\n\nFine-tune the model by providing a file with prompt and completion pairs\n\nThe challenge of the context-based approach is that models come with limited context size, and including all the documents as context may not fit into the allowed context size of the model. Depending on the model used, there may also be additional cost for larger context.\n\nFor the approach of fine-tuning, generating the right formatted information is time consuming and involves cost. In addition, if external data used for fine-tuning changes frequently, it would imply frequent fine-tunings and retraining are needed to create accurate results. Frequent training impacts speed to market and adds to the overall solution cost.\n\nTo demonstrate these constraints, we used an LLM Flan T5 XXL model and asked the following question:\n\nquestion = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n\nCode\n\nWe get the following response:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \nthe Managed Spot Training is a subscriptions product available for the following instances: Data Science Virtual Machine (DSVM), DSVM High, and DSVM Low.\n\"\"\"\n\nCode\n\nAs you can see, the response is not accurate. The correct answer should be all SageMaker instances support Managed Spot Training.\n\nWe tried the same question but with additional context passed along with the question:\n\nquestion + context + prompt = \"\"\"\nAnswer based on context:\n\nManaged Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n\nWhich instances can I use with Managed Spot Training in SageMaker?\n\"\"\"\n\nCode\n\nWe got the following response this time:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \ninstances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nThe response is better but still not accurate. However, in real production use cases, users may send various queries, and to provide accurate responses, you may want to include all or most of the available information as part of the static context to create accurate responses. Therefore, with this approach, we may hit the context size limitation constraint because even non-relevant information for the question asked is sent as part of the context. This is where you can use the RAG-based approach to create scalable and accurate responses for a user’s queries.\n\nRetrieval Augmented Generation\n\nTo solve the constraints we discussed, we can use Retrieval Augmented Generation (RAG) with LLMs. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. RAG models were introduced by\n\nLewis et al. in 2020\n\nas a model where parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\nIn RAG, the external data can come from multiple data sources, such as a document repository, databases, or APIs. The first step is to convert the documents and the user query in the format so they can be compared and relevancy search can be performed. To make the formats comparable for doing relevancy search, a document collection (knowledge library) and the user-submitted query are converted to numerical representation using embedding language models. The embeddings are essentially numerical representations of concept in text. Next, based on the embedding of user query, its relevant text is identified in the document collection by a similarity search in the embedding space. Then the prompt provided by the user is appended with relevant text that was searched and it’s added to the context. The prompt is now sent to the LLM and because the context has relevant external data along with the original prompt, the model output is relevant and accurate.\n\nTo maintain up-to-date information for the reference documents, you can asynchronously update the documents and update embedding representation of the documents. This way, the updated documents will be used to generate answers for future questions to provide accurate responses.\n\nThe following diagram shows the conceptual flow of using RAG with LLMs.\n\nIn this post, we demonstrate how to implement a question answering application with the following steps:\n\nGenerate embedding for each of document in the knowledge library with a SageMaker GPT-J-6B embedding model.\n\nIdentify the top K most relevant documents based on the user query.\n\nFor your query, generate the embedding of the query using the same embedding model.\n\nSearch the indexes of the top K most relevant documents in the embedding space using an in-memory FAISS search.\n\nUse the indexes to retrieve the corresponding documents.\n\nUse the retrieved relevant documents as context with the prompt and question, and send them to the SageMaker LLM to generate the response.\n\nWe demonstrate the following approaches:\n\nHow to solve a question answering task with SageMaker LLMs and embedding endpoints and the open-sourced library LangChain in a few lines of code. In particular, we use two SageMaker endpoints for the LLM (Flan T5 XXL) and embedding model (GPT-J 6B), and the vector database used is in-memory\n\nFAISS\n\n. For more details, see the\n\nGitHub repo\n\n.\n\nIf the in-memory FAISS doesn’t fit into your large dataset, we provide you with a\n\nSageMaker KNN algorithm\n\nto perform the semantic search, which also uses FAISS as the underlying searching algorithm. For details, see the\n\nGitHub repo\n\n.\n\nThe following diagram depicts the solution architecture.\n\nJumpStart RAG-based implementation notebook with LangChain\n\nLangChain\n\nis an open-source framework for developing applications powered by language models. LangChain provides a generic interface for many different LLMs. It also makes it easier for developers to chain various LLMs together and build powerful applications. LangChain provides a standard interface for memory and a collection of memory implementations to persist the state between calls of agents or chains.\n\nLangChain has many other utility features that can add to developer productivity. These features include a prompt template that helps customize prompts using variables in the prompt template, agents to build end-to-end applications, indexes for search and retrieval steps of the chain, and much more. To further explore LangChain capabilities, refer to the\n\nLangChain documentation\n\n.\n\nCreate LLM Model\n\nAs a first step, deploy the JumpStart LLM model of your choice. In this demo, we use a Jumpstart Flan T5 XXL model endpoint. For deployment instructions, refer to\n\nZero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart\n\n. Based on your use case, you can also deploy other instruction-tuned models like\n\nFlan T5 UL2\n\nor\n\nBloomZ 7B1\n\n. For details, see the\n\nexample notebook\n\n.\n\nTo use the SageMaker LLM endpoint with LangChain, we use\n\nlangchain.llms.sagemaker_endpoint.SagemakerEndpoint\n\n, which abstracts the SageMaker LLM endpoint. We need to perform a transformation for the request and response payload as shown in the following code for the LangChain SageMaker integration. Note that you may need to adjust the code in\n\nContentHandler\n\nbased on the\n\ncontent_type\n\nand accepts format of the LLM model that you choose to use.\n\nfrom\n\nlangchain\n\n.\n\nllms\n\n.\n\nsagemaker_endpoint\n\nimport\n\nSagemakerEndpoint\n\nclass\n\nContentHandler\n\n(\n\nContentHandlerBase\n\n)\n\n:\n\ncontent_type\n\n=\n\n\"application/json\"\n\naccepts\n\n=\n\n\"application/json\"\n\ndef\n\ntransform_input\n\n(\n\nself\n\n,\n\nprompt\n\n:\n\nstr\n\n,\n\nmodel_kwargs\n\n=\n\n{\n\n}\n\n)\n\n-\n\n>\n\nbytes\n\n:\n\ninput_str\n\n=\n\njson\n\n.\n\ndumps\n\n(\n\n{\n\n\"text_inputs\"\n\n:\n\nprompt\n\n,\n\n**\n\nmodel_kwargs\n\n}\n\n)\n\nreturn\n\ninput_str\n\n.\n\nencode\n\n(\n\n\"utf-8\"\n\n)\n\ndef\n\ntransform_output\n\n(\n\nself\n\n,\n\noutput\n\n:\n\nbytes\n\n)\n\n-\n\n>\n\nstr\n\n:\n\nresponse_json\n\n=\n\njson\n\n.\n\nloads\n\n(\n\noutput\n\n.\n\nread\n\n(\n\n)\n\n.\n\ndecode\n\n(\n\n\"utf-8\"\n\n)\n\n)\n\nreturn\n\nresponse_json\n\n[\n\n\"generated_texts\"\n\n]\n\n[\n\n0\n\n]\n\ncontent_handler\n\n=\n\nContentHandler\n\n(\n\n)\n\nsm_llm\n\n=\n\nSagemakerEndpoint\n\n(\n\nendpoint_name\n\n=\n\n_MODEL_CONFIG_\n\n[\n\n\"huggingface-text2text-flan-t5-xxl\"\n\n]\n\n[\n\n\"endpoint_name\"\n\n]\n\n,\n\nregion_name\n\n=\n\naws_region\n\n,\n\nmodel_kwargs\n\n=\n\nparameters\n\n,\n\ncontent_handler\n\n=\n\ncontent_handler\n\n,\n\n)\n\nPython\n\nCreate the embedding model\n\nNext, we need to get our embedded model ready. We deploy the\n\nGPT-J 6B\n\nmodel as the embedding model. If you’re using a JumpStart embedding model, you need to customize the LangChain SageMaker endpoint embedding class and transform the model request and response to integrate with LangChain. For a detailed implementation, refer to the\n\nGitHub repo\n\n.\n\nembeddings\n\n=\n\nSagemakerEndpointEmbeddingsJumpStart\n\n(\n\nendpoint_name\n\n=\n\n_MODEL_CONFIG_\n\n[\n\n\"huggingface-textembedding-gpt-j-6b\"\n\n]\n\n[\n\n\"endpoint_name\"\n\n]\n\n,\n\nregion_name\n\n=\n\naws_region\n\n,\n\ncontent_handler\n\n=\n\ncontent_handler\n\n,\n\n)\n\nPython\n\nLoad domain-specific documents using the LangChain document loader and create an index\n\nWe use the\n\nCSVLoader\n\npackage in LangChain to load CSV-formatted documents into the document loader:\n\nloader\n\n=\n\nCSVLoader\n\n(\n\nfile_path\n\n=\n\n\"rag_data/processed_data.csv\"\n\n)\n\ndocuments\n\n=\n\nloader\n\n.\n\nload\n\n(\n\n)\n\nPython\n\nNext, we use TextSplitter to preprocess data for embedding purposes and use the SageMaker embedding model\n\nGPT-J -6B\n\nto create the embedding. We store embedding in a FAISS vector store to create an index. We use this index to find relevant documents that are semantically similar to the user’s query.\n\nThe following code shows how all these steps are done by the\n\nVectorstoreIndexCreator\n\nclass in just few lines of code in LangChain to create a concise implementation of question answering with RAG:\n\nindex_creator\n\n=\n\nVectorstoreIndexCreator\n\n(\n\nvectorstore_cls\n\n=\n\nFAISS\n\n,\n\nembedding\n\n=\n\nembeddings\n\n,\n\ntext_splitter\n\n=\n\nCharacterTextSplitter\n\n(\n\nchunk_size\n\n=\n\n300\n\n,\n\nchunk_overlap\n\n=\n\n0\n\n)\n\n,\n\n)\n\nindex\n\n=\n\nindex_creator\n\n.\n\nfrom_loaders\n\n(\n\n[\n\nloader\n\n]\n\n)\n\nPython\n\nUse the index to search for relevant context and pass it to the LLM model\n\nNext, use the query method on the created index and pass the user’s question and SageMaker endpoint LLM. LangChain selects the top four closest documents (K=4) and passes the relevant context extracted from the documents to generate an accurate response. See the following code:\n\nindex.query(question=question, llm=sm_llm)\n\nCode\n\nWe get the following response for the query using the RAG-based approach with Flan T5 XXL:\n\n\"\"\"For model: huggingface-text2text-flan-t5-xxl, the generated output is: \nManaged Spot Training can be used with all instances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nThe response looks more accurate compared to the response we got with other approaches that we demonstrated earlier that have no context or static context that may not be always relevant.\n\nAlternate approach to implement RAG with more customization using SageMaker and LangChain\n\nIn this section, we show you another approach to implement RAG using SageMaker and LangChain. This approach offers the flexibility to configure top K parameters for a relevancy search in the documents. It also allows you to use the LangChain feature of\n\nprompt templates\n\n, which allow you to easily parameterize the prompt creation instead of hard coding the prompts.\n\nIn the following code, we explicitly use FAISS to generate embedding for each of the document in the knowledge library with the SageMaker GPT-J-6B embedding model. Then we identify the top K (K=3) most relevant documents based on the user query.\n\ndocsearch = FAISS.from_documents(documents, embeddings)\ndocs = docsearch.similarity_search(question, k=3)\n\nCode\n\nNext, we use a prompt template and chain it with the SageMaker LLM:\n\nprompt_template\n\n=\n\n\"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n\nPROMPT\n\n=\n\nPromptTemplate\n\n(\n\ntemplate\n\n=\n\nprompt_template\n\n,\n\ninput_variables\n\n=\n\n[\n\n\"context\"\n\n,\n\n\"question\"\n\n]\n\n)\n\nchain\n\n=\n\nload_qa_chain\n\n(\n\nllm\n\n=\n\nsm_llm\n\n,\n\nprompt\n\n=\n\nPROMPT\n\n)\n\nPython\n\nWe send the top three (K=3) relevant documents we found as context to the prompt by using a LangChain chain:\n\nresult\n\n=\n\nchain\n\n(\n\n{\n\n\"input_documents\"\n\n:\n\ndocs\n\n,\n\n\"question\"\n\n:\n\nquestion\n\n}\n\n,\n\nreturn_only_outputs\n\n=\n\nTrue\n\n)\n\n[\n\n\"output_text\"\n\n]\n\nPython\n\nWith this approach of RAG implementation, we were able to take advantage of the additional flexibility of LangChain prompt templates and customize the number of documents searched for a relevancy match using the top K hyperparameter.\n\nJumpStart RAG-based implementation notebook with SageMaker KNN\n\nIn this section, we implement the RAG-based approach using the KNN algorithm for finding relevant documents to create enhanced context. In this approach, we’re not using LangChain, but we use same dataset\n\nAmazon SageMaker FAQs\n\nas knowledge documents, embedding the models GPT-J-6B and LLM Flan T5 XXL just as we did in the previous LangChain approach.\n\nIf you have a large dataset, the\n\nSageMaker KNN algorithm\n\nmay provide you with an effective semantic search. The SageMaker KNN algorithm also uses FAISS as the underlying search algorithm. The notebook for this solution can be found on\n\nGitHub\n\n.\n\nFirst, we deploy the LLM Flan T5 XXL and GPT-J 6B embedding models in the same way as in the previous section. For each record in the knowledge database, we generate an embedding vector using the GPT-J embedding model.\n\nNext, we use a\n\nSageMaker KNN\n\ntraining job to index the embedding of the knowledge data. The underlying algorithm used to index the data is\n\nFAISS\n\n. We want to find the top five most relevant documents, so we set the\n\nTOP_K\n\nvariable to 5. We create the estimator for the KNN algorithm, run the training job, and deploy the KNN model to find indexes of the top five documents matching the query. See the following code:\n\nfrom\n\nsagemaker\n\n.\n\namazon\n\n.\n\namazon_estimator\n\nimport\n\nget_image_uri\n\ndef\n\ntrained_estimator_from_hyperparams\n\n(\n\ns3_train_data\n\n,\n\nhyperparams\n\n,\n\noutput_path\n\n)\n\n:\n\n\"\"\"\n    Create an Estimator from the given hyperparams, fit to training data,\n    and return a deployed predictor\n\n    \"\"\"\n\n# set up the estimator\n\nknn\n\n=\n\nsagemaker\n\n.\n\nestimator\n\n.\n\nEstimator\n\n(\n\nget_image_uri\n\n(\n\nboto3\n\n.\n\nSession\n\n(\n\n)\n\n.\n\nregion_name\n\n,\n\n\"knn\"\n\n)\n\n,\n\naws_role\n\n,\n\ninstance_count\n\n=\n\n1\n\n,\n\ninstance_type\n\n=\n\n\"ml.m5.2xlarge\"\n\n,\n\noutput_path\n\n=\n\noutput_path\n\n,\n\nsagemaker_session\n\n=\n\nsess\n\n,\n\n)\n\nknn\n\n.\n\nset_hyperparameters\n\n(\n\n**\n\nhyperparams\n\n)\n\n# train a model. fit_input contains the locations of the train data\n\nfit_input\n\n=\n\n{\n\n\"train\"\n\n:\n\ns3_train_data\n\n}\n\nknn\n\n.\n\nfit\n\n(\n\nfit_input\n\n)\n\nreturn\n\nknn\n\nhyperparams\n\n=\n\n{\n\n\"feature_dim\"\n\n:\n\ntrain_features\n\n.\n\nshape\n\n[\n\n1\n\n]\n\n,\n\n\"k\"\n\n:\n\nTOP_K\n\n,\n\n\"sample_size\"\n\n:\n\ntrain_features\n\n.\n\nshape\n\n[\n\n0\n\n]\n\n,\n\n\"predictor_type\"\n\n:\n\n\"classifier\"\n\n}\n\noutput_path\n\n=\n\nf\"s3://\n\n{\n\nbucket\n\n}\n\n/\n\n{\n\nprefix\n\n}\n\n/default_example/output\"\n\nknn_estimator\n\n=\n\ntrained_estimator_from_hyperparams\n\n(\n\ns3_train_data\n\n,\n\nhyperparams\n\n,\n\noutput_path\n\n)\n\nPython\n\nNext, we create an embedding representation of the query using the GPT-J-6B embedding model that we used for creating an embedding of the knowledge library documents:\n\nquery_response\n\n=\n\nquery_endpoint_with_json_payload\n\n(\n\nquestion\n\n,\n\nendpoint_name_embed\n\n,\n\ncontent_type\n\n=\n\n\"application/x-text\"\n\n)\n\nquestion_embedding\n\n=\n\nparse_response_text_embed\n\n(\n\nquery_response\n\n)\n\nPython\n\nThen we use the KNN endpoint and pass the embedding of the query to the KNN endpoint to get the indexes of the top K most relevant documents. We use the indexes to retrieve the corresponded textual documents. Next, we concatenate the documents, ensuring the maximum allowed length of context is not exceeded. See the following code:\n\n\"\"\"With maximum sequence length 500, selected top 4 document sections: \n  Managed Spot Training can be used with all instances supported in Amazon SageMaker.\n  Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n  The difference between Savings Plans for Amazon SageMaker and Savings Plans for EC2 is in the services they \n  include. \n  SageMaker Savings Plans apply only to SageMaker ML Instance usage.\n  There are no fixed limits to the size of the dataset you can use for training models with Amazon SageMaker.\n\"\"\"\n\nCode\n\nNow we come to our final step in which we combine the query, prompt, and the context containing text from relevant documents and pass it to the text generation LLM Flan T5 XXL model to generate the answer.\n\nWe get the following response for the query using a RAG-based approach with Flan T5 XXL:\n\n\"\"\"\nFor model: huggingface-text2text-flan-t5-xxl, the generated output is: \n\nManaged Spot Training can be used with all instances supported in Amazon SageMaker\n\"\"\"\n\nCode\n\nClean up\n\nMake sure to delete the endpoints that we created in this notebook when not using them to avoid reoccurring cost.\n\nConclusion\n\nIn this post, we demonstrated the implementation of a RAG-based approach with LLMs for question answering tasks using two approaches: LangChain and the built-in KNN algorithm. The RAG-based approach optimizes the accuracy of the text generation using Flan T5 XXL by dynamically providing relevant context that was created by searching a list of documents.\n\nYou can use this these notebooks in SageMaker as is or you may customize them to your needs. To customize, you can use your own set of documents in the knowledge library, use other relevancy search implementations like OpenSearch, and use other embedding models and text generation LLMs available on JumpStart.\n\nWe look forward to seeing what you build on JumpStart using a RAG-based approach!\n\nAbout the authors\n\nDr. Xin Huang\n\nis a Senior Applied Scientist for Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He focuses on developing scalable machine learning algorithms. His research interests are in the area of natural language processing, explainable deep learning on tabular data, and robust analysis of non-parametric space-time clustering. He has published many papers in ACL, ICDM, KDD conferences, and Royal Statistical Society: Series A.\n\nRachna Chadha\n\nis a Principal Solution Architect AI/ML in Strategic Accounts at AWS. Rachna is an optimist who believes that ethical and responsible use of AI can improve society in future and bring economical and social prosperity. In her spare time, Rachna likes spending time with her family, hiking and listening to music.\n\nDr. Kyle Ulrich\n\nis an Applied Scientist with the Amazon SageMaker built-in algorithms team. His research interests include scalable machine learning algorithms, computer vision, time series, Bayesian non-parametrics, and Gaussian processes. His PhD is from Duke University and he has published papers in NeurIPS, Cell, and Neuron.\n\nHemant Singh\n\nis a Machine Learning Engineer with experience in Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He got his masters from Courant Institute of Mathematical Sciences and B.Tech from IIT Delhi. He had experience in working on a diverse range of Machine Learning problems within the domain of natural language processing, computer vision, and time-series analysis.\n\nManas Dadarkar\n\nis a Software Development Manager owning the engineering of the Amazon Forecast service. He is passionate about the applications of machine learning and making ML technologies easily available for everyone to adopt and deploy to production. Outside of work, he has multiple interests including travelling, reading and spending time with friends and family.\n\nDr. Ashish Khetan\n\nis a Senior Applied Scientist with Amazon SageMaker built-in algorithms and helps develop machine learning algorithms. He got his PhD from University of Illinois Urbana-Champaign. He is an active researcher in machine learning and statistical inference, and has published many papers in NeurIPS, ICML, ICLR, JMLR, ACL, and EMNLP conferences.\n\nLike\n\n(0)\n\nShare\n\nComments\n\nLog in to comment\n\nLog in\n\nAWS Podcast\n\nSubscribe for weekly AWS news and interviews\n\nLearn more\n\nAWS Partner Network\n\nFind an APN member to support your cloud business needs\n\nLearn more\n\nAWS Training & Certifications\n\nFree digital courses to help you develop your skills\n\nLearn more",
    "abstract": "Today, we announce the availability of sample notebooks that demonstrate question answering tasks using a Retrieval Augmented Generation (RAG)-based approach with large language models (LLMs) in Amazon SageMaker JumpStart. Text generation using RAG with LLMs enables you to generate domain-specific text outputs by supplying specific external data as part of the context fed to LLMs. […]",
    "first_paragraph": "Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart",
    "retrieved_at": "2025-11-01T16:09:25.185266",
    "content_type": "blog",
    "content_length": 22793
}