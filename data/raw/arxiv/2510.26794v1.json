{
    "paper_id": "2510.26794v1",
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation",
    "authors": [
        "Jing Lin",
        "Ruisi Wang",
        "Junzhe Lu",
        "Ziqi Huang",
        "Guorui Song",
        "Ailing Zeng",
        "Xian Liu",
        "Chen Wei",
        "Wanqi Yin",
        "Qingping Sun",
        "Zhongang Cai",
        "Lei Yang",
        "Ziwei Liu"
    ],
    "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
    "published_date": "2025-10-30T17:59:27+00:00",
    "updated_date": "2025-10-30T17:59:27+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.26794v1",
    "categories": [
        "cs.CV"
    ],
    "retrieved_at": "2025-10-31T14:31:36.920448"
}