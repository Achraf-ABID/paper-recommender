"""
Script d'√©valuation des r√©sum√©s g√©n√©r√©s avec ROUGE et BERTScore
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import evaluate
import pandas as pd
from tqdm import tqdm
import json

# ==========================================
# 1. CONFIGURATION
# ==========================================

# Chemins du mod√®le fine-tun√©
# Chemins du mod√®le fine-tun√©
MODEL_PATH = "../models/bart-lora-finetuned"  # Chemin vers votre mod√®le LoRA
# MODEL_PATH = "facebook/bart-large-cnn"  # Fallback

# Fichier de donn√©es de test (format: articles + r√©sum√©s de r√©f√©rence)
TEST_DATA_PATH = "../test_data_example.json"  # Chemin corrig√©

# Param√®tres de g√©n√©ration
MAX_INPUT_LENGTH = 1024
MAX_OUTPUT_LENGTH = 150
NUM_BEAMS = 4

# ==========================================
# 2. CHARGEMENT DES M√âTRIQUES
# ==========================================

print("üìä Chargement des m√©triques d'√©valuation...")

# ROUGE: mesure le chevauchement de n-grammes
rouge = evaluate.load("rouge")

# BERTScore: mesure la similarit√© s√©mantique
bertscore = evaluate.load("bertscore")

# ==========================================
# 3. CHARGEMENT DU MOD√àLE
# ==========================================

from peft import PeftModel, PeftConfig

print(f"ü§ñ Chargement du mod√®le: {MODEL_PATH}")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üíª Utilisation de: {device}")

# Chargement sp√©cifique pour LoRA
try:
    config = PeftConfig.from_pretrained(MODEL_PATH)
    base_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
    model = PeftModel.from_pretrained(base_model, MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    print("‚úÖ Mod√®le LoRA charg√© avec succ√®s")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur chargement LoRA, essai chargement standard: {e}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)

model.to(device)
model.eval()

# ==========================================
# 4. FONCTION DE G√âN√âRATION DE R√âSUM√â
# ==========================================


def generate_summary(article_text):
    """
    G√©n√®re un r√©sum√© pour un article donn√©

    Args:
        article_text (str): Texte de l'article

    Returns:
        str: R√©sum√© g√©n√©r√©
    """
    # Tokenisation
    inputs = tokenizer(
        article_text,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    ).to(device)

    # G√©n√©ration
    with torch.no_grad():
        summary_ids = model.generate(
            input_ids=inputs["input_ids"],  # Argument nomm√© obligatoire pour PEFT
            max_length=MAX_OUTPUT_LENGTH,
            num_beams=NUM_BEAMS,
            length_penalty=2.0,
            early_stopping=True,
        )

    # D√©codage
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary


# ==========================================
# 5. FONCTION D'√âVALUATION
# ==========================================


def evaluate_summaries(test_data):
    """
    √âvalue les r√©sum√©s g√©n√©r√©s par rapport aux r√©sum√©s de r√©f√©rence

    Args:
        test_data (list): Liste de dictionnaires avec 'article' et 'reference_summary'

    Returns:
        dict: Scores ROUGE et BERTScore
    """
    generated_summaries = []
    reference_summaries = []

    print("\nüîÑ G√©n√©ration des r√©sum√©s...")

    for item in tqdm(test_data):
        article = item["article"]
        reference = item["reference_summary"]

        # G√©n√©rer le r√©sum√©
        generated = generate_summary(article)

        generated_summaries.append(generated)
        reference_summaries.append(reference)

    print("\nüìà Calcul des scores ROUGE...")

    # Calcul ROUGE
    rouge_scores = rouge.compute(
        predictions=generated_summaries, references=reference_summaries
    )

    print("\nüìà Calcul des scores BERTScore...")

    # Calcul BERTScore
    bertscore_results = bertscore.compute(
        predictions=generated_summaries,
        references=reference_summaries,
        lang="en",  # Changez en "fr" si vos r√©sum√©s sont en fran√ßais
    )

    # Moyennes BERTScore
    avg_bertscore = {
        "precision": sum(bertscore_results["precision"])
        / len(bertscore_results["precision"]),
        "recall": sum(bertscore_results["recall"]) / len(bertscore_results["recall"]),
        "f1": sum(bertscore_results["f1"]) / len(bertscore_results["f1"]),
    }

    return {
        "rouge": rouge_scores,
        "bertscore": avg_bertscore,
        "generated_summaries": generated_summaries,
        "reference_summaries": reference_summaries,
    }


# ==========================================
# 6. AFFICHAGE DES R√âSULTATS
# ==========================================


def display_results(results):
    """
    Affiche les r√©sultats d'√©valuation de mani√®re lisible
    """
    print("\n" + "=" * 50)
    print("üìä R√âSULTATS D'√âVALUATION")
    print("=" * 50)

    # ROUGE Scores
    print("\nüî¥ ROUGE Scores:")
    print(f"  ROUGE-1: {results['rouge']['rouge1']:.4f}")
    print(f"  ROUGE-2: {results['rouge']['rouge2']:.4f}")
    print(f"  ROUGE-L: {results['rouge']['rougeL']:.4f}")

    # BERTScore
    print("\nüü¢ BERTScore:")
    print(f"  Precision: {results['bertscore']['precision']:.4f}")
    print(f"  Recall:    {results['bertscore']['recall']:.4f}")
    print(f"  F1:        {results['bertscore']['f1']:.4f}")

    print("\n" + "=" * 50)


# ==========================================
# 7. FONCTION PRINCIPALE
# ==========================================


def main():
    """
    Fonction principale pour ex√©cuter l'√©valuation
    """
    print("üöÄ D√©but de l'√©valuation des r√©sum√©s\n")

    # Charger les donn√©es de test
    print(f"üìÇ Chargement des donn√©es: {TEST_DATA_PATH}")

    # EXEMPLE DE FORMAT ATTENDU pour test_data.json:
    # [
    #   {
    #     "article": "Long article text here...",
    #     "reference_summary": "Reference summary here..."
    #   },
    #   ...
    # ]

    try:
        with open(TEST_DATA_PATH, "r", encoding="utf-8") as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Fichier non trouv√©: {TEST_DATA_PATH}")
        print("\nüí° Cr√©ez un fichier JSON avec ce format:")
        print("""
[
  {
    "article": "Votre article complet ici...",
    "reference_summary": "Le r√©sum√© de r√©f√©rence ici..."
  }
]
        """)
        return

    print(f"‚úÖ {len(test_data)} exemples charg√©s\n")

    # √âvaluer
    results = evaluate_summaries(test_data)

    # Afficher les r√©sultats
    display_results(results)

    # Sauvegarder les r√©sultats
    output_file = "../results/evaluation_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "rouge": {
                    "rouge1": results["rouge"]["rouge1"],
                    "rouge2": results["rouge"]["rouge2"],
                    "rougeL": results["rouge"]["rougeL"],
                },
                "bertscore": results["bertscore"],
            },
            f,
            indent=2,
        )

    print(f"\nüíæ R√©sultats sauvegard√©s dans: {output_file}")


# ==========================================
# EXEMPLE D'UTILISATION AVEC VOS PROPRES DONN√âES
# ==========================================


def evaluate_single_summary(article, reference_summary):
    """
    √âvalue un seul r√©sum√© (utile pour tester rapidement)

    Args:
        article (str): Texte de l'article
        reference_summary (str): R√©sum√© de r√©f√©rence
    """
    generated = generate_summary(article)

    rouge_scores = rouge.compute(
        predictions=[generated], references=[reference_summary]
    )

    bertscore_results = bertscore.compute(
        predictions=[generated], references=[reference_summary], lang="en"
    )

    print("\nüìÑ R√©sum√© g√©n√©r√©:")
    print(generated)
    print("\nüìÑ R√©sum√© de r√©f√©rence:")
    print(reference_summary)
    print("\nüìä Scores:")
    print(f"ROUGE-1: {rouge_scores['rouge1'].mid.fmeasure:.4f}")
    print(f"ROUGE-2: {rouge_scores['rouge2'].mid.fmeasure:.4f}")
    print(f"BERTScore F1: {bertscore_results['f1'][0]:.4f}")


# ==========================================
# EX√âCUTION
# ==========================================

if __name__ == "__main__":
    main()

    # Ou pour tester avec un seul exemple:
    # evaluate_single_summary(
    #     article="Your article text...",
    #     reference_summary="Your reference summary..."
    # )
