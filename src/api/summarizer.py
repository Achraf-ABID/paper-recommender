"""
Module de r√©sum√© avec le mod√®le LoRA fine-tun√©
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel, PeftConfig
import torch
from typing import List, Optional


class LoRASummarizer:
    """Classe pour g√©rer le r√©sum√© avec le mod√®le LoRA"""

    def __init__(self, model_path: str):
        """
        Initialise le r√©sumeur avec le mod√®le LoRA

        Args:
            model_path: Chemin vers le dossier du mod√®le LoRA
        """
        print(f"ü§ñ Chargement du mod√®le de r√©sum√© depuis {model_path}...")

        try:
            # Charger la configuration LoRA
            config = PeftConfig.from_pretrained(model_path)

            # Charger le mod√®le de base
            base_model = AutoModelForSeq2SeqLM.from_pretrained(
                config.base_model_name_or_path
            )

            # Charger les adaptateurs LoRA
            self.model = PeftModel.from_pretrained(base_model, model_path)

            # Charger le tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)

            # D√©tecter le device
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            self.model.eval()

            print(f"‚úÖ Mod√®le charg√© sur {self.device.upper()}")

        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise

    def summarize_single(
        self,
        text: str,
        max_length: int = 150,  # Augment√© pour plus de d√©tails
        min_length: int = 50,
        num_beams: int = 4,  # Augment√© pour la qualit√©
        length_penalty: float = 2.0,
    ) -> str:
        """
        R√©sume un seul article avec des param√®tres optimis√©s pour la qualit√©
        """
        # Technique 2: Prompt Engineering (Contextualisation)
        prompt = f"Summarize the following technical article concisely:\n\n{text}"

        # Tokenizer
        inputs = self.tokenizer(
            prompt, return_tensors="pt", max_length=1024, truncation=True
        ).to(self.device)

        # G√©n√©ration avec param√®tres anti-r√©p√©tition (Technique 1)
        with torch.no_grad():
            summary_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                max_length=max_length,
                min_length=min_length,
                length_penalty=length_penalty,
                num_beams=num_beams,
                no_repeat_ngram_size=3,  # Emp√™che les r√©p√©titions de 3 mots
                repetition_penalty=1.2,  # Punit les r√©p√©titions
                early_stopping=True,
            )

        # D√©coder
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary

    def summarize_multiple(
        self,
        articles: List[dict],
        individual_max_length: int = 120,
        global_max_length: int = 250,
    ) -> dict:
        """
        R√©sume plusieurs articles avec une strat√©gie Map-Reduce am√©lior√©e
        """
        individual_summaries = []

        # 1. R√©sumer chaque article individuellement
        for article in articles:
            # On combine titre et abstract pour plus de contexte
            text = f"Title: {article.get('title', '')}\nContent: {article.get('abstract', '')}"

            summary = self.summarize_single(
                text, max_length=individual_max_length, min_length=40
            )
            individual_summaries.append(
                {
                    "title": article.get("title"),
                    "summary": summary,
                    "source": article.get("source"),
                    "url": article.get("url"),
                }
            )

        # 2. Cr√©er un r√©sum√© global (Technique 3: Strat√©gie am√©lior√©e)
        # On utilise les r√©sum√©s individuels comme base
        combined_text = " ".join(
            [
                f"Source {i + 1}: {summ['summary']}"
                for i, summ in enumerate(individual_summaries)
            ]
        )

        # Prompt sp√©cifique pour la synth√®se
        synthesis_prompt = f"Synthesize these summaries into a single coherent technical overview:\n\n{combined_text}"

        global_summary = self.summarize_single(
            synthesis_prompt,
            max_length=global_max_length,
            min_length=100,
            num_beams=5,  # Qualit√© maximale pour le r√©sum√© final
            length_penalty=2.5,
        )

        return {
            "individual_summaries": individual_summaries,
            "global_summary": global_summary,
            "total_articles": len(articles),
        }
